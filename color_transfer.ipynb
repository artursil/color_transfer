{
    "cells": [
        {
            "metadata": {
                "trusted": true
            },
            "id": "9519f654",
            "cell_type": "code",
            "source": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport math\nimport fastai\nfrom PIL import Image\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nfrom dataloader import get_imagenette_dataloader\nfrom quantize import quantize_img, plot_imgs\nfrom ddpm import DDPMCB\nfrom preprocessing import clip_preprocess, conditioning_transform\nfrom functools import partial\nfrom fastai.vision.all import (ImageDataLoaders, Resize, TensorImage, Learner, \n                               Callback, Normalize)\nfrom encoder import ViTImageEncoder\nimport fastcore.all as fc\n\ndevice = \"cuda\"",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "27bc2a31",
            "cell_type": "code",
            "source": "def method_helper(o): return list(filter(lambda x: x[0] != \"_\", dir(o)))",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "e5e55aab",
            "cell_type": "code",
            "source": "stage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", \n     torch_dtype=torch.float16, class_labels=None \n)",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "\nA mixture of fp16 and non-fp16 filenames will be loaded.\nLoaded fp16 filenames:\n[text_encoder/model.fp16-00001-of-00002.safetensors, text_encoder/model.fp16-00002-of-00002.safetensors, safety_checker/model.fp16.safetensors, unet/diffusion_pytorch_model.fp16.safetensors]\nLoaded non-fp16 filenames:\n[watermarker/diffusion_pytorch_model.safetensors\nIf this behavior is not expected, please check your folder structure.\nKeyword arguments {'class_labels': None} are not expected by IFSuperResolutionPipeline and will be ignored.\n",
                    "name": "stderr"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "ee9d54c0ceba486b8f086bbb88003a68"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "text": "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "8adf7f2e",
            "cell_type": "code",
            "source": "scheduler = stage_2.scheduler\nunet = stage_2.unet.to(device)",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "be1e0413",
            "cell_type": "code",
            "source": "dls = ImageDataLoaders.from_folder( \"/mnt/wd/datasets/imagenette2\", valid_pct=0.1, bs=1,)\none_batch = dls.one_batch()[0]\none_batch.shape",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 5,
                    "data": {
                        "text/plain": "torch.Size([1, 3, 334, 500])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "dca0078a",
            "cell_type": "code",
            "source": "encoder = ViTImageEncoder(7, output_dim=unet.config.encoder_hid_dim).to(device)\nencoder_preprocess = encoder.feature_extractor\nc_preprocess = partial(clip_preprocess, stage_2=stage_2)\ncond_transform = partial(conditioning_transform, encode_preprocess=None)",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "ff61d8cb",
            "cell_type": "code",
            "source": "def preprocessing(x):\n    if not isinstance(x, fastai.vision.core.TensorCategory):\n        x = Resize(224)(x)\n        x = TensorImage(x).permute(2,1,0)\n    x = cond_transform(x)\n    x = x.to(\"cpu\")\n    x = c_preprocess(x)\n    return x",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "8463a4f5",
            "cell_type": "code",
            "source": "dls = ImageDataLoaders.from_folder(\n    \"/mnt/wd/datasets/imagenette2\",\n    valid_pct=0.1,\n    item_tfms=[preprocessing],\n    # batch_tfms=[Normalize()],\n    bs=4,\n    num_workers=16\n)",
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "> \u001b[0;32m/tmp/ipykernel_16954/4111660116.py\u001b[0m(9)\u001b[0;36mpreprocessing\u001b[0;34m()\u001b[0m\n\u001b[0;32m      5 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7 \u001b[0;31m    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\nipdb> x.shape\ntorch.Size([10, 224, 224])\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "d0c316fa",
            "cell_type": "code",
            "source": "dls.one_batch()[0].shape\ndls.one_batch()[0][0,2,...].std()",
            "execution_count": 30,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "> \u001b[0;32m/home/artursil/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/image_utils.py\u001b[0m(255)\u001b[0;36minfer_channel_dimension_format\u001b[0;34m()\u001b[0m\n\u001b[0;32m    253 \u001b[0;31m    \u001b[0;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_dim\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLAST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 255 \u001b[0;31m    \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to infer channel dimension format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\nipdb> u\n> \u001b[0;32m/home/artursil/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/models/clip/image_processing_clip.py\u001b[0m(320)\u001b[0;36mpreprocess\u001b[0;34m()\u001b[0m\n\u001b[0;32m    318 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0minput_data_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319 \u001b[0;31m            \u001b[0;31m# We assume that all images have the same channel dimension format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 320 \u001b[0;31m            \u001b[0minput_data_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_channel_dimension_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322 \u001b[0;31m        \u001b[0mall_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\nipdb> u\n> \u001b[0;32m/home/artursil/anaconda3/envs/ai/lib/python3.12/site-packages/transformers/image_processing_utils.py\u001b[0m(41)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[0;32m     39 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40 \u001b[0;31m        \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m---> 41 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\nipdb> u\n> \u001b[0;32m/mnt/wd/color_transfer/preprocessing.py\u001b[0m(88)\u001b[0;36mclip_preprocess\u001b[0;34m()\u001b[0m\n\u001b[0;32m     86 \u001b[0;31m    \u001b[0mclip_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87 \u001b[0;31m    \u001b[0mgt_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m---> 88 \u001b[0;31m    \u001b[0menc_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89 \u001b[0;31m    \u001b[0mgt_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\nipdb> frame.shape\ntorch.Size([10, 224, 224])\nipdb> enc_img = frame[3:,...]\nipdb> enc_img.shape\ntorch.Size([7, 224, 224])\nipdb> n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "87478145",
            "cell_type": "code",
            "source": "class CTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.unet = unet\n        self.unet.class_embedding = None\n        self.vit = ViTImageEncoder(7, output_dim=self.unet.config.encoder_hid_dim).to(device)\n\n        for param in self.unet.parameters():\n            param.requires_grad = False\n        \n\n    def forward(self, noisy_images, images, t):\n        encoded = self.vit(images).expand(-1, 77, -1).half()\n\n        return self.unet(noisy_images.half(), t.half(), encoded.half())[0]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "cee00b8d",
            "cell_type": "code",
            "source": "model = CTModel()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "bd829c5f",
            "cell_type": "code",
            "source": "one_batch = dls.one_batch()\none_batch[0].shape\nimages = one_batch[0]\n# images = torch.cat([images, images], dim=1)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "b9594662",
            "cell_type": "code",
            "source": "# Without DDPM callback it won't work\n# with torch.no_grad():\n#     x = model(images, one_batch[0], torch.tensor([1.0]*4, dtype=torch.float16, device=\"cuda\"))\n# x",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "72973a95",
            "cell_type": "code",
            "source": "learn = Learner(dls, model, loss_func=torch.nn.MSELoss(), cbs=[DDPMCB(unet,scheduler)]).to_fp16()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "bab5f0b6",
            "cell_type": "code",
            "source": "from fastai.callback.hook import ActivationStats\n\n# Create a list of layers to track. You can add or remove layers based on what you want to observe.\nlayers_to_track = [\n    learn.model.vit.vit.embeddings.patch_embeddings.projection,\n    learn.model.vit.vit.encoder.layer[0].attention.attention.query,\n    learn.model.vit.vit.encoder.layer[0].attention.attention.key,\n    learn.model.vit.vit.encoder.layer[0].attention.attention.value,\n    learn.model.vit.vit.encoder.layer[0].intermediate.dense,\n    learn.model.vit.vit.encoder.layer[0].output.dense,\n    learn.model.vit.vit.encoder.layer[0].layernorm_before,\n    learn.model.vit.vit.encoder.layer[0].layernorm_after,\n    learn.model.vit.vit.encoder.layer[6].attention.attention.query,\n    learn.model.vit.vit.encoder.layer[6].attention.attention.key,\n    learn.model.vit.vit.encoder.layer[6].attention.attention.value,\n    learn.model.vit.vit.encoder.layer[6].intermediate.dense,\n    learn.model.vit.vit.encoder.layer[6].output.dense,\n    learn.model.vit.vit.encoder.layer[6].layernorm_before,\n    learn.model.vit.vit.encoder.layer[6].layernorm_after,\n    learn.model.vit.vit.layernorm,\n    learn.model.vit.vit.pooler.dense,\n]\n\n# Add the ActivationStats callback\nastats = ActivationStats(modules=layers_to_track)\nlearn.add_cb(astats)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "137e6e15",
            "cell_type": "code",
            "source": "learn.lr_find()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "bdae41a9",
            "cell_type": "code",
            "source": "lr = 10e-04\nlearn.fit_one_cycle(1, lr)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "677536fb",
            "cell_type": "code",
            "source": "learn.save(\"ctransfer_epoch_1.pth\")\n# learn = learn.load(\"ctransfer_epoch_1.pth\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "7b366505",
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "1b207194",
            "cell_type": "code",
            "source": "# learn2 = learn.add_cb(astats)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "99490714",
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "f6b596f7",
            "cell_type": "code",
            "source": "# astats.color_dim()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "77401192",
            "cell_type": "code",
            "source": "lr = 10e-05\n# learn.fit_one_cycle(1, lr)\nlearn.load(\"ctransfer_epoch_2.pth\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "75569855",
            "cell_type": "code",
            "source": "lr = 10e-05\n# learn.save(\"ctransfer_epoch_2.pth\")\nlearn.fit_one_cycle(4, lr)\nlearn.save(\"ctransfer_epoch_3_6.pth\")\n# learn.save(\"ctransfer_epoch_4.pth\")\n# learn.fit_one_cycle(1, lr)\n# learn.save(\"ctransfer_epoch_5.pth\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "e0944fe2",
            "cell_type": "code",
            "source": "learn.fit_one_cycle(3, lr)\nlearn.save(\"ctransfer_epoch_6_8.pth\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "675132b7",
            "cell_type": "code",
            "source": "lr = 10e-06\nlearn.fit_one_cycle(1, lr)\nlearn.save(\"ctransfer_epoch_9.pth\")",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}