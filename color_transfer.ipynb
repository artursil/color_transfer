{
    "cells": [
        {
            "metadata": {
                "trusted": true
            },
            "id": "273bbf00",
            "cell_type": "code",
            "source": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport math\nimport fastai\nfrom PIL import Image\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nfrom dataloader import get_imagenette_dataloader\nfrom quantize import quantize_img, plot_imgs\nfrom ddpm import DDPMCB\nfrom preprocessing import clip_preprocess, conditioning_transform\nfrom functools import partial\nfrom fastai.vision.all import ImageDataLoaders, Resize, TensorImage, Learner, Callback\nfrom encoder import ViTImageEncoder\nimport fastcore.all as fc\n\ndevice = \"cuda\"",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "0f9f1dc9",
            "cell_type": "code",
            "source": "def method_helper(o): return list(filter(lambda x: x[0] != \"_\", dir(o)))",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "7f9dea16",
            "cell_type": "code",
            "source": "# stage g\nstage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", \n     torch_dtype=torch.float16, class_labels=None \n)\n# stage_2.enable_model_cpu_offload()",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "\nA mixture of fp16 and non-fp16 filenames will be loaded.\nLoaded fp16 filenames:\n[text_encoder/model.fp16-00002-of-00002.safetensors, text_encoder/model.fp16-00001-of-00002.safetensors, safety_checker/model.fp16.safetensors, unet/diffusion_pytorch_model.fp16.safetensors]\nLoaded non-fp16 filenames:\n[watermarker/diffusion_pytorch_model.safetensors\nIf this behavior is not expected, please check your folder structure.\nKeyword arguments {'class_labels': None} are not expected by IFSuperResolutionPipeline and will be ignored.\n",
                    "name": "stderr"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "ac4b852e1b684268a88890402d28fae8"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "text": "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "fc431545",
            "cell_type": "code",
            "source": "scheduler = stage_2.scheduler\nunet = stage_2.unet.to(device)",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "2224b6a7",
            "cell_type": "code",
            "source": "dls = ImageDataLoaders.from_folder( \"/mnt/wd/datasets/imagenette2\", valid_pct=0.1, bs=1,)\none_batch = dls.one_batch()[0]\none_batch.shape",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 5,
                    "data": {
                        "text/plain": "torch.Size([1, 3, 384, 512])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "cfaa5b63",
            "cell_type": "code",
            "source": "encoder = ViTImageEncoder(7, output_dim=unet.config.encoder_hid_dim).to(device)\nencoder_preprocess = encoder.feature_extractor\nc_preprocess = partial(clip_preprocess, stage_2=stage_2)",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "86324830",
            "cell_type": "code",
            "source": "img = c_preprocess(one_batch[0].cpu().numpy())",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "8f486588",
            "cell_type": "code",
            "source": "cond_transform = partial(conditioning_transform, encode_preprocess=None)\ncond_transform(img).shape",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "torch.Size([10, 224, 224])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "0e132ac0",
            "cell_type": "code",
            "source": "def preprocessing(x):\n    if isinstance(x, fastai.vision.core.PILImage):\n        x = TensorImage(x).permute(2,1,0).numpy()\n    x = c_preprocess(x)\n    return cond_transform(x)\n\npreprocessing(one_batch[0].cpu().numpy()).shape",
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 9,
                    "data": {
                        "text/plain": "torch.Size([10, 224, 224])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "eed8d5e5",
            "cell_type": "code",
            "source": "dls = ImageDataLoaders.from_folder(\n    \"/mnt/wd/datasets/imagenette2\",\n    valid_pct=0.1,\n    item_tfms=[Resize(224), preprocessing],\n    bs=4,\n    num_workers=16\n)",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "e0577eac",
            "cell_type": "code",
            "source": "dls.one_batch()[0].shape",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 11,
                    "data": {
                        "text/plain": "torch.Size([4, 10, 224, 224])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "13899ea6",
            "cell_type": "code",
            "source": "class CTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.unet = unet\n        self.unet.class_embedding = None\n        self.vit = ViTImageEncoder(7, output_dim=self.unet.config.encoder_hid_dim).to(device)\n\n        for param in self.unet.parameters():\n            param.requires_grad = False\n        \n\n    def forward(self, noisy_images, images, t):\n        encoded = self.vit(images).expand(-1, 77, -1).half()\n\n        return self.unet(noisy_images.half(), t.half(), encoded.half())[0]",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "f03de851",
            "cell_type": "code",
            "source": "model = CTModel()",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "ef9401fd",
            "cell_type": "code",
            "source": "one_batch = dls.one_batch()\none_batch[0].shape\nimages = one_batch[0]\n# images = torch.cat([images, images], dim=1)",
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "059bdc2c",
            "cell_type": "code",
            "source": "# Without DDPM callback it won't work\n# with torch.no_grad():\n#     x = model(images, one_batch[0], torch.tensor([1.0]*4, dtype=torch.float16, device=\"cuda\"))\n# x",
            "execution_count": 15,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "de8a8773",
            "cell_type": "code",
            "source": "learn = Learner(dls, model, loss_func=torch.nn.MSELoss(), cbs=[DDPMCB(unet,scheduler)]).to_fp16()\n# learn = Learner(dls, model.half(), loss_func=torch.nn.MSELoss(), cbs=[DDPMCB(unet,scheduler)])\n# from fastai.learner import AvgSmoothLoss\n\n# class FP16AvgSmoothLoss(AvgSmoothLoss):\n#     def accumulate(self, learn):\n#         self.count += 1\n#         loss_fp16 = to_detach(learn.loss.mean()).half()  # Ensure FP16\n#         self.val = torch.lerp(loss_fp16, self.val.half(), self.beta)  # Convert self.val to FP16\n\n# learn.recorder.metrics = []\n\n# learn.lr_find()",
            "execution_count": 16,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "5be3b5ce",
            "cell_type": "code",
            "source": "lr = 10e-05\n# learn.fit_one_cycle(1, lr)\n# learn.save(\"ctransfer_epoch_1.pth\")\nlearn = learn.load(\"ctransfer_epoch_1.pth\")",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "/home/artursil/anaconda3/envs/ai/lib/python3.12/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "c019b5a9",
            "cell_type": "code",
            "source": "from collections.abc import Mapping\nclass Hook():\n    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n    def remove(self): self.hook.remove()\n    def __del__(self): self.remove()\n\n\nclass Hooks(list):\n    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n    def __enter__(self, *args): return self\n    def __exit__ (self, *args): self.remove()\n    def __del__(self): self.remove()\n    def __delitem__(self, i):\n        self[i].remove()\n        super().__delitem__(i)\n    def remove(self):\n        for h in self: h.remove()\n\n\nclass HooksCallback(Callback):\n    def __init__(self, hookfunc, mod_filter=fc.noop, on_train=True, on_valid=False, mods=None):\n        fc.store_attr()\n        super().__init__()\n    \n    def before_fit(self):\n        learn = self.learn\n        if self.mods: mods=self.mods\n        else: mods = fc.filter_ex(learn.model.modules(), self.mod_filter)\n        self.hooks = Hooks(mods, partial(self._hookfunc, learn))\n\n    def _hookfunc(self, learn, *args, **kwargs):\n        if (self.on_train and learn.training) or (self.on_valid and not learn.training): self.hookfunc(*args, **kwargs)\n\n    def after_fit(self): self.learn.hooks.remove()\n    def __iter__(self): return iter(self.hooks)\n    def __len__(self): return len(self.hooks)\n\ndef to_cpu(x):\n    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n    if isinstance(x, list): return [to_cpu(o) for o in x]\n    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n    res = x.detach().cpu()\n    return res.float() if res.dtype==torch.float16 else res\n\ndef append_stats(hook, mod, inp, outp):\n    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n    acts = to_cpu(outp)\n    hook.stats[0].append(acts.mean())\n    hook.stats[1].append(acts.std())\n    hook.stats[2].append(acts.abs().histc(40,0,10))\n\nclass ActivationStats(HooksCallback):\n    def __init__(self, mod_filter=fc.noop): \n        super().__init__(hookfunc=append_stats, mod_filter=mod_filter)\n\n    def color_dim(self, figsize=(11,5)):\n        fig,axes = get_grid(len(self), figsize=figsize)\n        for ax,h in zip(axes.flat, self):\n            show_image(get_hist(h), ax, origin='lower')\n\n    def dead_chart(self, figsize=(11,5)):\n        fig,axes = get_grid(len(self), figsize=figsize)\n        for ax,h in zip(axes.flatten(), self):\n            ax.plot(get_min(h))\n            ax.set_ylim(0,1)\n\n    def plot_stats(self, figsize=(10,4)):\n        fig,axs = plt.subplots(1,2, figsize=figsize)\n        for h in self:\n            for i in 0,1: axs[i].plot(h.stats[i])\n        axs[0].set_title('Means')\n        axs[1].set_title('Stdevs')\n        plt.legend(fc.L.range(self))\n        \nastats = ActivationStats(fc.risinstance(\"Linear\"))",
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "AttributeError",
                    "evalue": "module 'fastcore' has no attribute 'noop'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mremove\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m: h\u001b[38;5;241m.\u001b[39mremove()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mHooksCallback\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mCallback\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhookfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_valid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[18], line 21\u001b[0m, in \u001b[0;36mHooksCallback\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mHooksCallback\u001b[39;00m(Callback):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hookfunc, mod_filter\u001b[38;5;241m=\u001b[39m\u001b[43mfc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoop\u001b[49m, on_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_valid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, mods\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m         fc\u001b[38;5;241m.\u001b[39mstore_attr()\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
                        "\u001b[0;31mAttributeError\u001b[0m: module 'fastcore' has no attribute 'noop'"
                    ]
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "22c2cb9c",
            "cell_type": "code",
            "source": "learn2 = learn.add_cb(astats)\nlearn2.save(\"ctransfer_epoch_2.pth\")",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "985fb6a7",
            "cell_type": "code",
            "source": "import matplotlib.pyplot as plt\ndef subplots(\n    nrows:int=1, # Number of rows in returned axes grid\n    ncols:int=1, # Number of columns in returned axes grid\n    figsize:tuple=None, # Width, height in inches of the returned figure\n    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n    suptitle:str=None, # Title to be set to returned figure\n    **kwargs\n): # fig and axs\n    \"A figure and set of subplots to display images of `imsize` inches\"\n    if figsize is None: figsize=(ncols*imsize, nrows*imsize)\n    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n    if suptitle is not None: fig.suptitle(suptitle)\n    if nrows*ncols==1: ax = np.array([ax])\n    return fig,ax\n\n\ndef show_image(im, ax=None, figsize=None, title=None, noframe=True, **kwargs):\n    \"Show a PIL or PyTorch image on `ax`.\"\n    if fc.hasattrs(im, ('cpu','permute','detach')):\n        im = im.detach().cpu()\n        if len(im.shape)==3 and im.shape[0]<5: im=im.permute(1,2,0)\n    elif not isinstance(im,np.ndarray): im=np.array(im)\n    if im.shape[-1]==1: im=im[...,0]\n    if ax is None: _,ax = plt.subplots(figsize=figsize)\n    ax.imshow(im, **kwargs)\n    if title is not None: ax.set_title(title)\n    ax.set_xticks([]) \n    ax.set_yticks([]) \n    if noframe: ax.axis('off')\n    return ax\n\ndef get_grid(\n    n:int, # Number of axes\n    nrows:int=None, # Number of rows, defaulting to `int(math.sqrt(n))`\n    ncols:int=None, # Number of columns, defaulting to `ceil(n/rows)`\n    title:str=None, # If passed, title set to the figure\n    weight:str='bold', # Title font weight\n    size:int=14, # Title font size\n    **kwargs,\n): # fig and axs\n    \"Return a grid of `n` axes, `rows` by `cols`\"\n    if nrows: ncols = ncols or int(np.floor(n/nrows))\n    elif ncols: nrows = nrows or int(np.ceil(n/ncols))\n    else:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.floor(n/nrows))\n    fig,axs = subplots(nrows, ncols, **kwargs)\n    for i in range(n, nrows*ncols): axs.flat[i].set_axis_off()\n    if title is not None: fig.suptitle(title, weight=weight, size=size)\n    return fig,axs\n\n\ndef get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "c781ccb0",
            "cell_type": "code",
            "source": "astats.color_dim()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "b0b8f01e",
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "f4cac9a1",
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}