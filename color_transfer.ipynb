{
    "cells": [
        {
            "metadata": {
                "trusted": true
            },
            "id": "273bbf00",
            "cell_type": "code",
            "source": "import numpy as np\nimport torch\nimport math\nimport fastai\nfrom PIL import Image\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import pt_to_pil\nfrom dataloader import get_imagenette_dataloader\nfrom quantize import quantize_img, plot_imgs\nfrom ddpm import DDPMCB\nfrom preprocessing import clip_preprocess, conditioning_transform\nfrom functools import partial\nfrom fastai.vision.all import ImageDataLoaders\nfrom encoder import ViTImageEncoder\n\ndevice = \"cuda\"",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "0f9f1dc9",
            "cell_type": "code",
            "source": "def method_helper(o): return list(filter(lambda x: x[0] != \"_\", dir(o)))",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "2224b6a7",
            "cell_type": "code",
            "source": "encoder = ViTImageEncoder(7, output_dim=unet.config.encoder_hid_dim).to(device)\nencoder_preprocess = encoder.feature_extractor",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "1ee9b229",
            "cell_type": "code",
            "source": "# stage 2\nstage_2 = DiffusionPipeline.from_pretrained(\n    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", \n     torch_dtype=torch.float16, class_labels=None \n)\n# stage_2.enable_model_cpu_offload()",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "\nA mixture of fp16 and non-fp16 filenames will be loaded.\nLoaded fp16 filenames:\n[text_encoder/model.fp16-00002-of-00002.safetensors, text_encoder/model.fp16-00001-of-00002.safetensors, safety_checker/model.fp16.safetensors, unet/diffusion_pytorch_model.fp16.safetensors]\nLoaded non-fp16 filenames:\n[watermarker/diffusion_pytorch_model.safetensors\nIf this behavior is not expected, please check your folder structure.\nKeyword arguments {'class_labels': None} are not expected by IFSuperResolutionPipeline and will be ignored.\n",
                    "name": "stderr"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "8c11b6b7855c41e6a8f9f507564f7f13"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "text": "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "6e16e9a1",
            "cell_type": "code",
            "source": "scheduler = stage_2.scheduler\n\n# Load the UNet model - the core denoiser\nunet = stage_2.unet.to(device)",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "fccf6511",
            "cell_type": "code",
            "source": "clip_preprocess = partial(clip_preprocess, stage_2=stage_2)",
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "86324830",
            "cell_type": "code",
            "source": "dls = ImageDataLoaders.from_folder(\n    \"/mnt/wd/datasets/imagenette2\",\n    valid_pct=0.1,\n    item_tfms=[clip_preprocess, conditioning_transform],\n    bs=4,\n    num_workers=16\n)",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "70b542e1",
            "cell_type": "code",
            "source": "dls.one_batch()[0].shape",
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 9,
                    "data": {
                        "text/plain": "torch.Size([4, 7, 224, 224])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "3d4c57d1",
            "cell_type": "code",
            "source": "# dls.one_batch()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "c7daac64",
            "cell_type": "code",
            "source": "# dls = ImageDataLoaders.from_folder(\"/mnt/wd/datasets/imagenette2\", valid_pct=0.1, bs=4, item_tfms=Resize(224))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "d20d717b",
            "cell_type": "code",
            "source": "# def preprocess(frame)\n#     return clip_processor(frame)[\"pixel_values\"][0]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "c6b0f79d",
            "cell_type": "code",
            "source": "# clip_processor(dls.one_batch()[0], rescale=False)[\"pixel_values\"][0].shape",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "13899ea6",
            "cell_type": "code",
            "source": "one_batch = dls.one_batch()[0]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "f03de851",
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "ef9401fd",
            "cell_type": "code",
            "source": "",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "059bdc2c",
            "cell_type": "code",
            "source": "",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 11,
                    "data": {
                        "text/plain": "ViTImageProcessor {\n  \"do_convert_rgb\": null,\n  \"do_normalize\": true,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"height\": 224,\n    \"width\": 224\n  }\n}"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "da518a14",
            "cell_type": "code",
            "source": "encoder(dls.one_batch()[0])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "de8a8773",
            "cell_type": "code",
            "source": "class CTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.unet = unet\n        self.unet.class_embedding = None\n        self.vit = ViTImageEncoder(7, output_dim=self.unet.config.encoder_hid_dim).to(device)\n\n        for param in self.unet.parameters():\n            param.requires_grad = False\n        \n\n    def forward(self, noisy_images, images, t):\n        encoded = self.vit(images).expand(-1, 77, -1).half()\n\n        return self.unet(noisy_images.half(), t.half(), encoded.half())[0]",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "fa5358fb",
            "cell_type": "code",
            "source": "model = CTModel()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "aef4257f",
            "cell_type": "code",
            "source": "one_batch = dls.one_batch()\none_batch[0].shape\nimages = one_batch[0]\nimages = torch.cat([images, images], dim=1)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "2f344cbc",
            "cell_type": "code",
            "source": "with torch.no_grad():\n    x = model(images, one_batch[0], torch.tensor([1.0]*4, dtype=torch.float16, device=\"cuda\"))\n# x",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "ea0039ae",
            "cell_type": "code",
            "source": "x[0].size()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "5a0a26f2",
            "cell_type": "code",
            "source": "learn = Learner(dls, model, loss_func=torch.nn.MSELoss(), cbs=[DDPMCB(unet,scheduler)]).to_fp16()\n# learn = Learner(dls, model.half(), loss_func=torch.nn.MSELoss(), cbs=[DDPMCB(unet,scheduler)])\n# from fastai.learner import AvgSmoothLoss\n\n# class FP16AvgSmoothLoss(AvgSmoothLoss):\n#     def accumulate(self, learn):\n#         self.count += 1\n#         loss_fp16 = to_detach(learn.loss.mean()).half()  # Ensure FP16\n#         self.val = torch.lerp(loss_fp16, self.val.half(), self.beta)  # Convert self.val to FP16\n\n# learn.recorder.metrics = []\n\nlearn.lr_find()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "82fb81f6",
            "cell_type": "code",
            "source": "lr = 10e-05\nlearn.fit_one_cycle(1, lr)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "4a8d8a36",
            "cell_type": "code",
            "source": "# If lr_max is not provided, use the suggested learning rate from the finder\n    lr_max = lr_max or lr_max_suggested\n    print(f\"Using learning rate: {lr_max:.2e}\")\n\n    # \ud83d\ude80 Step 2: Train the model with OneCycle policy\n    learn.fit_one_cycle(epochs, lr_max)\n\n    return learn  # Return trained Learner",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "id": "afb9a7ef",
            "cell_type": "code",
            "source": "x = x.expand(-1, 77, -1)\none_batch = torch.cat([one_batch, one_batch], dim=1)",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}