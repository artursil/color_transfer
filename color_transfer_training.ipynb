{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613f1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import fastai\n",
    "from PIL import Image\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.utils import pt_to_pil\n",
    "from dataloader import get_imagenette_dataloader\n",
    "from quantize import quantize_img, plot_imgs\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7950edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_helper(o): return list(filter(lambda x: x[0] != \"_\", dir(o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2bc0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A mixture of fp16 and non-fp16 filenames will be loaded.\n",
      "Loaded fp16 filenames:\n",
      "[unet/diffusion_pytorch_model.fp16.safetensors, safety_checker/model.fp16.safetensors, text_encoder/model.fp16-00001-of-00002.safetensors, text_encoder/model.fp16-00002-of-00002.safetensors]\n",
      "Loaded non-fp16 filenames:\n",
      "[watermarker/diffusion_pytorch_model.safetensors\n",
      "If this behavior is not expected, please check your folder structure.\n",
      "Keyword arguments {'class_labels': None} are not expected by IFSuperResolutionPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf2798b8097489380789041475e64a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# stage 2\n",
    "stage_2 = DiffusionPipeline.from_pretrained(\n",
    "    \"DeepFloyd/IF-II-L-v1.0\", text_encoder=None, variant=\"fp16\", torch_dtype=torch.float16, class_labels=None \n",
    ")\n",
    "# stage_2.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4eaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = stage_2.scheduler\n",
    "\n",
    "# Load the UNet model - the core denoiser\n",
    "unet = stage_2.unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafca7c-ca20-4992-b188-c54c5f0e3c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a978d0-f61d-4561-a8b5-529d93e06d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import Callback, ImageDataLoaders, Resize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DDPMCB(Callback):\n",
    "    \"\"\"Custom FastAI callback for training a UNet diffusion model with DeepFloyd's DDPM Scheduler.\"\"\"\n",
    "\n",
    "    def __init__(self, unet, scheduler, timesteps=1000):\n",
    "        self.unet = unet\n",
    "        self.scheduler = scheduler\n",
    "        self.timesteps = timesteps  # Number of diffusion steps\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"\"\"Add noise to the input images before passing them to the model.\"\"\"\n",
    "        # Get the real images from the batch\n",
    "        images = self.xb[0]  # Shape: (batch_size, 3, H, W)\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Generate noise\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=images.device).long()\n",
    "        noise = torch.randn_like(images)[:,:3,...]\n",
    "\n",
    "        # Apply noise\n",
    "        noisy_images = self.scheduler.add_noise(images[:,:3,...], noise, t)\n",
    "\n",
    "        noisy_images = torch.cat([noisy_images, noisy_images], dim=1)  # Shape: (batch_size, 6, H, W)\n",
    "        gt_image_true = torch.cat([images[:,:3,...], images[:,:3,...]], dim=1)  # Shape: (batch_size, 6, H, W)\n",
    " \n",
    "        self.learn.xb = (noisy_images, images, t)\n",
    "        self.learn.yb = (gt_image_true,)\n",
    "\n",
    "    def after_pred(self):\n",
    "        \"\"\"Compute loss: Train UNet to predict noise (MSE loss).\"\"\"\n",
    "        gt_image_pred = self.pred.to(torch.float32)  # Model's prediction\n",
    "        gt_image_true = self.yb[0]  # Ground truth (now 6-channel)\n",
    "        \n",
    "        self.learn.loss = nn.functional.mse_loss(gt_image_pred, gt_image_true)\n",
    "\n",
    "    # def after_batch(self):\n",
    "    #     \"\"\"Zero gradients manually after each batch (recommended for diffusion).\"\"\"\n",
    "    #     self.learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0267c10b-2f6f-41e3-ba3c-998a3691052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_category(f):\n",
    "    def wrapper(frame):\n",
    "        if not isinstance(frame, TensorCategory):\n",
    "            frame = f(frame)\n",
    "        return frame\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9cdaac-a94c-4e7c-b3ae-601794f878c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision.transforms import functional as TF\n",
    "import cv2\n",
    "\n",
    "\n",
    "def quantize_image(image: Image.Image, num_colors: int) -> Image.Image:\n",
    "    \"\"\"Quantizes a PIL image using the median cut algorithm.\"\"\"\n",
    "    return image.convert(\"RGB\").quantize(colors=num_colors, method=Image.MEDIANCUT).convert(\"RGB\")\n",
    "\n",
    "def compute_luminance(image: Image.Image) -> Image.Image:\n",
    "    \"\"\"Computes the luminance (grayscale) channel of a PIL image.\"\"\"\n",
    "    return ImageOps.grayscale(image)\n",
    "\n",
    "def compute_gradients(luminance: Image.Image) -> tuple:\n",
    "    \"\"\"Computes x- and y-direction gradients of the luminance channel.\"\"\"\n",
    "    lum_array = np.array(luminance).astype(np.float32)\n",
    "    grad_x = cv2.Sobel(lum_array, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(lum_array, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    return grad_x, grad_y\n",
    "\n",
    "def threshold_gradients(grad_x, grad_y, threshold=8) -> tuple:\n",
    "    \"\"\"Thresholds gradients to create binary images.\"\"\"\n",
    "    grad_x = (np.abs(grad_x) > threshold).astype(np.float32)\n",
    "    grad_y = (np.abs(grad_y) > threshold).astype(np.float32)\n",
    "    return grad_x, grad_y\n",
    "\n",
    "@ignore_category\n",
    "def conditioning_transform(image_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies all transformatiqons to generate the 7-channel input.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A tensor image of shape (C, H, W) in range [0, 1].\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A 7-channel tensor of shape (7, H, W) in range [0, 1].\n",
    "    \"\"\"\n",
    "    # Convert tensor to PIL image for processing\n",
    "    image = TF.to_pil_image(image_tensor)\n",
    "\n",
    "    # 1-3: Quantized Image\n",
    "    num_colors = 2 ** np.random.randint(2, 8)\n",
    "    quantized_image = quantize_image(image, num_colors)\n",
    "    quantized_tensor = TF.to_tensor(quantized_image)\n",
    "\n",
    "    # 4: Quantization Level Channel (normalized to [0,1])\n",
    "    quant_level_channel = torch.full((1, quantized_tensor.shape[1], quantized_tensor.shape[2]), num_colors / 256)\n",
    "\n",
    "    # 5: Luminance Channel\n",
    "    luminance = compute_luminance(image)\n",
    "    luminance_tensor = TF.to_tensor(luminance)\n",
    "\n",
    "    # 6: Gradient-Based Conditioning\n",
    "    grad_x, grad_y = compute_gradients(luminance)\n",
    "    grad_x_tensor = torch.tensor(grad_x, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "    grad_y_tensor = torch.tensor(grad_y, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "    # 7: Texture Indicator (1 if texture is present, 0 otherwise)\n",
    "    texture_indicator = torch.ones_like(quant_level_channel)\n",
    "\n",
    "    # Stack all channels into a 7-channel tensor\n",
    "    stacked_tensor = torch.cat([quantized_tensor, quant_level_channel, grad_x_tensor, grad_y_tensor, texture_indicator], dim=0)\n",
    "\n",
    "    return stacked_tensor.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616ddea2-2f4f-4172-b6b7-47775559e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor = stage_2.feature_extractor\n",
    "\n",
    "@ignore_category\n",
    "def clip_preprocess(frame):\n",
    "    return torch.tensor(clip_processor(frame)[\"pixel_values\"][0])\n",
    "    \n",
    "@ignore_category\n",
    "def to_f16(frame):\n",
    "    torch.tensor(frame, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d541e2-8351-4b67-8d35-8141df114bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "dls = ImageDataLoaders.from_folder(\n",
    "    \"/mnt/wd/datasets/imagenette2\",\n",
    "    valid_pct=0.1,\n",
    "    item_tfms=[clip_preprocess, conditioning_transform],\n",
    "    bs=4,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2120b9e-f5a6-417a-aa7f-0aa6c0509e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626ac455-f4b3-4d3d-8601-9d5d9a1d958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7b7754-3422-4cf9-b4d5-05c910622955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls = ImageDataLoaders.from_folder(\"/mnt/wd/datasets/imagenette2\", valid_pct=0.1, bs=4, item_tfms=Resize(224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a7cf3c-9dec-4de2-a324-469dd95ab1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(frame)\n",
    "#     return clip_processor(frame)[\"pixel_values\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2961717e-39fc-4d7c-bb0d-a11a12fbe22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_processor(dls.one_batch()[0], rescale=False)[\"pixel_values\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d248b5b2-7f23-45cb-8585-1567df872880",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = dls.one_batch()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2c979-833e-4bcd-b483-25e18a0dcef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e089124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTImageProcessor\n",
    "class ViTImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses a pre-trained ViT model to extract embeddings from a quantized image.\n",
    "    This replaces the CNN encoder with a stronger transformer-based encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self,channels_in,  model_name=\"google/vit-base-patch16-224\", output_dim=1024):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "        self.feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "        self.fc = nn.Linear(self.vit.config.hidden_size, output_dim)  # Resize to match UNet's expected size\n",
    "        self.vit.embeddings.patch_embeddings.projection = nn.Conv2d(channels_in, 768, kernel_size=(16, 16), stride=(16, 16))\n",
    "        self.vit.config.num_channels = channels_in\n",
    "        self.vit.embeddings.patch_embeddings.num_channels = channels_in\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.vit(x).last_hidden_state  # Extract token embeddings\n",
    "        pooled_features = features.mean(dim=1)  # Global Average Pooling (B, D)\n",
    "        return self.fc(pooled_features).unsqueeze(1)  # Shape: (batch, 1, output_dim)\n",
    "\n",
    "# =============================\n",
    "# 2️⃣ Load DeepFloyd IF UNet & Scheduler\n",
    "# =============================\n",
    "\n",
    "\n",
    "encoder = ViTImageEncoder(7, output_dim=unet.config.encoder_hid_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4cc827b-56bb-4d9c-94c7-cb30e86236a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1545,  0.8223,  0.3134,  ...,  0.5701,  0.5903, -0.5687]],\n",
       "\n",
       "        [[ 0.0820,  0.7439,  0.3273,  ...,  0.4758,  0.4894, -0.4048]],\n",
       "\n",
       "        [[ 0.1745,  0.8123,  0.1718,  ...,  0.4505,  0.5863, -0.4291]],\n",
       "\n",
       "        [[ 0.0928,  0.8939,  0.2339,  ...,  0.5358,  0.4837, -0.5152]]],\n",
       "       device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(dls.one_batch()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29085dcb-0a45-40bf-9910-a7032448ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.unet.class_embedding = None\n",
    "        self.vit = ViTImageEncoder(7, output_dim=self.unet.config.encoder_hid_dim).to(device)\n",
    "\n",
    "        for param in self.unet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "\n",
    "    def forward(self, noisy_images, images, t):\n",
    "        encoded = self.vit(images).expand(-1, 77, -1).half()\n",
    "\n",
    "        return self.unet(noisy_images.half(), t.half(), encoded.half())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea4a88dc-a318-443f-afed-c8f6cf4734a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a4d1fb-bfa6-447e-a21a-99d4ee85bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = dls.one_batch()\n",
    "one_batch[0].shape\n",
    "images = one_batch[0]\n",
    "images = torch.cat([images, images], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f91f772e-d2b2-4e27-82eb-ae7bc7a1e488",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [160, 6, 3, 3], expected input[4, 14, 224, 224] to have 6 channels, but got 14 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# x\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mCTModel.forward\u001b[0;34m(self, noisy_images, images, t)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noisy_images, images, t):\n\u001b[1;32m     13\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit(images)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m77\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_condition.py:1169\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1164\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_encoder_hidden_states(\n\u001b[1;32m   1165\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states, added_cond_kwargs\u001b[38;5;241m=\u001b[39madded_cond_kwargs\n\u001b[1;32m   1166\u001b[0m )\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# 2. pre-process\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;66;03m# 2.5 GLIGEN position net\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [160, 6, 3, 3], expected input[4, 14, 224, 224] to have 6 channels, but got 14 channels instead"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = model(images, one_batch[0], torch.tensor([1.0]*4, dtype=torch.float16, device=\"cuda\"))\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d72a34-d119-4698-afac-9ac967f807f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaf452-120b-4199-affd-2ee114866f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=torch.nn.MSELoss(), cbs=[DDPMCB(unet,scheduler)]).to_fp16()\n",
    "# learn = Learner(dls, model.half(), loss_func=torch.nn.MSELoss(), cbs=[DDPMCB(unet,scheduler)])\n",
    "# from fastai.learner import AvgSmoothLoss\n",
    "\n",
    "# class FP16AvgSmoothLoss(AvgSmoothLoss):\n",
    "#     def accumulate(self, learn):\n",
    "#         self.count += 1\n",
    "#         loss_fp16 = to_detach(learn.loss.mean()).half()  # Ensure FP16\n",
    "#         self.val = torch.lerp(loss_fp16, self.val.half(), self.beta)  # Convert self.val to FP16\n",
    "\n",
    "# learn.recorder.metrics = []\n",
    "\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31867c7b-fc53-4abd-a33b-4d3d1c8c2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10e-05\n",
    "learn.fit_one_cycle(1, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707f666-c9dd-47f8-bbe0-c7170140eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If lr_max is not provided, use the suggested learning rate from the finder\n",
    "    lr_max = lr_max or lr_max_suggested\n",
    "    print(f\"Using learning rate: {lr_max:.2e}\")\n",
    "\n",
    "    # 🚀 Step 2: Train the model with OneCycle policy\n",
    "    learn.fit_one_cycle(epochs, lr_max)\n",
    "\n",
    "    return learn  # Return trained Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188dd06e-1587-45a8-b78c-fe2fe5ab6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.expand(-1, 77, -1)\n",
    "one_batch = torch.cat([one_batch, one_batch], dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
